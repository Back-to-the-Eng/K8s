apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-app-code
  namespace: default
data:
  user_activity_streaming.py: |
    import re
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, hour, when, lit, regexp_extract, substring, udf
    from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, TimestampType

    spark = (
        SparkSession.builder
        .appName("KafkaToMinIO_Silver")
        .getOrCreate()
    )

    hadoop_conf = spark._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.endpoint", "http://minio.storage.svc.cluster.local:9000")
    hadoop_conf.set("fs.s3a.access.key", "admin")
    hadoop_conf.set("fs.s3a.secret.key", "password1234")
    hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    hadoop_conf.set("fs.s3a.path.style.access", "true")

    hadoop_conf.setInt("fs.s3a.connection.timeout", 60000)
    hadoop_conf.setInt("fs.s3a.connection.establish.timeout", 60000)
    hadoop_conf.setInt("fs.s3a.attempts.maximum", 10)

    spark.sparkContext.setLogLevel("WARN")

    # UDF 정의 (로컬 코드와 동일)
    def extract_action_type(log):
        if log is None:
            return None
        tokens = re.split(r"\] ?", log)
        if len(tokens) < 2:
            return None
        seg = tokens[1].strip().split(" ")
        if seg[0] == "-":
            return seg[1]
        return seg[0]

    def extract_endpoint(log):
        if log is None:
            return None
        match = re.search(r"(GET|POST|PUT|DELETE)\s+(\S+)", log)
        if match:
            return match.group(2)
        return None

    extract_action_type_udf = udf(extract_action_type, StringType())
    extract_endpoint_udf = udf(extract_endpoint, StringType())

    # Kafka에서 데이터 읽기
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "streaming-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092") \
        .option("subscribe", "user-activity-logs") \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()

    # Bronze 레이어: Raw JSON + Metadata
    bronze_df = df.selectExpr(
        "CAST(value AS STRING) AS raw_json",
        "topic",
        "partition",
        "offset",
        "CAST(timestamp AS STRING) AS kafka_timestamp"
    )

    # raw_json 정리 (로컬 코드와 동일)
    cleaned_df = bronze_df.withColumn(
        "raw_json_clean",
        regexp_extract(col("raw_json"), r"^.*?(\[[A-Z_]+.*)", 1)
    )

    # Silver 레이어: 정규표현식으로 파싱 (로컬 코드와 동일)
    parsed_df = (
        cleaned_df
        .withColumn("event_type", regexp_extract(col("raw_json_clean"), r"\[(.*?)\]", 1))
        .withColumn("user_id", regexp_extract(col("raw_json_clean"), r"\b(\d{3,6})\b", 1).cast(IntegerType()))
        .withColumn("endpoint", extract_endpoint_udf(col("raw_json_clean")))
        .withColumn("action_type", extract_action_type_udf(col("raw_json_clean")))
        .withColumn(
            "event_time_raw",
            regexp_extract(col("raw_json_clean"), r"(20\d{2}-\d{2}-\d{2}T[\d:\.]+)", 1)
        )
    )

    # Timestamp 안정화 (로컬 코드와 동일: 26자 포맷)
    parsed_df = parsed_df.withColumn("event_time_trim", substring(col("event_time_raw"), 1, 26))
    parsed_df = parsed_df.withColumn(
        "event_time",
        to_timestamp(col("event_time_trim"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS")
    )

    # HTTP 메서드 및 상태 추출 (로컬 코드와 동일)
    silver_df = (
        parsed_df
        .withColumn(
            "http_method",
            regexp_extract(col("raw_json_clean"), r"\b(GET|POST|PUT|DELETE)\b", 1)
        )
        .withColumn(
            "status",
            regexp_extract(col("raw_json_clean"), r"\b(API_CALL|API_FINISH|DATA_LOOKUP|FAILED)\b", 1)
        )
        .withColumn("http_method", when(col("http_method") == "", lit(None)).otherwise(col("http_method")))
        .withColumn("status", when(col("status") == "", lit(None)).otherwise(col("status")))
        .withColumn("is_failed",
            when(col("status") == lit("FAILED"), lit(1))
            .otherwise(lit(0))
            .cast(IntegerType())
        )
        .withColumn("year", year(col("event_time")))
        .withColumn("month", month(col("event_time")))
        .withColumn("day", dayofmonth(col("event_time")))
        .withColumn("hour", hour(col("event_time")))
        .select(
            col("raw_json"),
            "topic",
            "partition",
            "offset",
            "kafka_timestamp",
            col("raw_json_clean"),
            "http_method",
            "status",
            "is_failed",
            "event_type",
            "user_id",
            "endpoint",
            "action_type",
            "event_time",
            "year",
            "month",
            "day",
            "hour"
        )
    )

    # Bronze 레이어는 MinIO에 저장하지 않음 (메모리에서만 처리)
    # Bronze → Silver는 직접 메모리 전달

    # Silver 레이어를 Parquet 형식으로 저장
    silver_query = (
        silver_df.writeStream
        .format("parquet")
        .option("path", "s3a://silver/user-activity-v2/")
        .option("checkpointLocation", "s3a://silver/checkpoints/silver_v1")
        .partitionBy("year", "month", "day", "hour")
        .outputMode("append")
        .start()
    )

    # Silver 스트림 실행
    silver_query.awaitTermination()


