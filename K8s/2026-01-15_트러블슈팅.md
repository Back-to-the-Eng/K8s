# 2026-01-15 트러블슈팅 및 작업 내역

## 1. 작업 개요

| 항목 | 내용 |
|------|------|
| 날짜 | 2026-01-15 |
| 작업자 | 김도윤 |
| 주요 작업 | Bronze 저장 추가, 체크포인트 S3 마이그레이션, 리소스 최적화 |

---

## 2. 트러블슈팅 내역

### 2.1 Bronze 레이어 저장 누락 수정

**문제:**
- `spark-code-configmap.yaml`에서 `bronze_df` 변수를 만들기만 하고 실제로 MinIO에 저장하지 않고 있었음
- Silver만 저장되고 Bronze(raw-data)는 저장 안 됨

**원인:**
- 설계 시 Bronze 저장 코드 누락 (PVC 작업과 무관, 처음부터 없었음)
- 커밋 `a22b7c1` (1월 8일)에서 bronze_df 변수만 추가하고 저장 코드 미작성

**해결:**
```python
# Bronze 레이어를 JSON 형식으로 저장 (Raw 데이터 보존)
bronze_query = (
    bronze_df.writeStream
    .format("json")
    .option("path", "s3a://raw-data/user-activity/")
    .option("checkpointLocation", "s3a://raw-data/checkpoints/bronze_v1")
    .partitionBy("topic")
    .outputMode("append")
    .start()
)
```

**변경 파일:** `K8s/spark-code-configmap.yaml`

---

### 2.2 체크포인트 PVC → S3 마이그레이션

**문제:**
```
java.io.IOException: mkdir of file:/app/checkpoints/gold_clickhouse_spark351_prod_v5 failed
```

**원인:**
- EBS PVC는 특정 노드에 바인딩됨 (RWO: ReadWriteOnce)
- Pod가 다른 노드에 스케줄링되면 PVC 접근 불가
- Spark Operator가 PVC 마운트를 제대로 처리 못함

**해결:**
```python
# 변경 전
gold_checkpoint = "/app/checkpoints/gold_clickhouse_spark351_prod_v5"  # PVC

# 변경 후
gold_checkpoint = "s3a://gold/checkpoints/gold_v7"  # S3 (MinIO)
```

**변경 파일:** 
- `K8s/spark-gold-code-configmap.yaml`
- `K8s/spark-code-configmap.yaml` (Silver 체크포인트도 S3로 변경)

---

### 2.3 Ivy 캐시 에러 수정

**문제:**
```
FileNotFoundException: /home/spark/.ivy2.5.2/cache/...
```

**원인:**
- Spark 컨테이너에서 `/home/spark/.ivy2` 디렉토리에 쓰기 권한 없음

**해결:**
```yaml
sparkConf:
  "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
  "spark.executor.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
  "spark.jars.ivy": "/tmp/.ivy2"
```

**변경 파일:** `K8s/spark-gold-application.yaml`

---

### 2.4 Executor 리소스 최적화

**문제:**
```
0/4 nodes are available: 4 Insufficient cpu
```

**원인:**
- m6i.large 인스턴스는 2 vCPU
- cores: 2로 설정하면 Executor가 스케줄링 안 됨

**해결:**
```yaml
# 변경 전
executor:
  cores: 2
  memory: "4g"
  instances: 3

# 변경 후
executor:
  cores: 1
  memory: "2g"
  instances: 2  # m6i.large 리소스 제약
```

**변경 파일:** `K8s/spark-gold-application.yaml`

---

### 2.5 백로그 문제 인지

**현상:**
```
WARN: Current batch is falling behind. 
trigger: 60초, 실제 소요: 858초 (14분)
```

**원인:**
- Spark Streaming 재시작 동안 Kafka에 메시지 계속 쌓임
- 약 4500만 건 백로그 발생
- 백로그 처리에 리소스 집중 → 실시간성 저하

**해결 방안:**
- Silver 버킷 삭제 → 체크포인트 리셋 → `startingOffsets: "latest"` 적용
- Kafka 데이터는 보존 (원본 유지)

---

## 3. 현재 시스템 상태

### 3.1 Pod 상태 (모두 Running)

| 컴포넌트 | Namespace | Pod 수 | 상태 | 리소스 제한 |
|----------|-----------|--------|------|------------|
| WAS | was | 3 | Running ✅ | CPU: 200m-1000m, Memory: 512Mi-1Gi |
| Kafka Broker | kafka | 3 | Running ✅ | CPU: 200m-500m, Memory: 1Gi-2Gi |
| Kafka Zookeeper | kafka | 3 | Running ✅ | CPU: 100m-300m, Memory: 512Mi-1Gi |
| Spark Streaming | default | 1 | PENDING_RERUN ⚠️ | - |
| Spark Gold Driver | default | 1 | Running ✅ | - |
| Spark Gold Executor | default | 2 | Running ✅ | - |
| ClickHouse | storage | 1 | Running ✅ | PVC: 20Gi |
| MinIO | storage | 1 | Running ✅ | PVC: 20Gi |

### 3.2 데이터 흐름

```
WAS (3 replicas)
    ↓
Kafka (3 brokers)
    ↓
Spark Streaming
    ├── Bronze (s3a://raw-data/user-activity/) ✅
    └── Silver (s3a://silver/user-activity-v2/) ✅
            ↓
      Spark Gold
            ↓
      ClickHouse (5개 테이블)
```

### 3.3 MinIO 버킷 구조

```
MinIO
├── raw-data/           # Bronze 레이어
│   ├── user-activity/  # JSON 파일
│   └── checkpoints/    # Bronze 체크포인트
├── silver/             # Silver 레이어
│   ├── user-activity-v2/  # Parquet 파일
│   └── checkpoints/    # Silver 체크포인트
└── gold/               # Gold 레이어
    └── checkpoints/    # Gold 체크포인트
```

---

## 4. 커밋 정보

### 4.1 초기 커밋
```
commit e1feed4
Author: 김도윤
Date: 2026-01-15

fix: Bronze 레이어 저장 추가 및 체크포인트 S3 마이그레이션

변경 파일:
- K8s/spark-code-configmap.yaml
- K8s/spark-gold-application.yaml
- K8s/spark-gold-code-configmap.yaml
```

### 4.2 추가 커밋
```
commit 710c9e2
fix: ClickHouse 스키마 퍼널 테이블 수정 (metrics_funnel_hourly → metrics_funnel_daily)

commit e939d34
fix: 리소스 제한 추가 및 Spark Streaming 설정 수정
- WAS CPU/Memory 리소스 제한 추가
- Kafka Broker/Zookeeper 리소스 제한 추가
- Spark Streaming Kafka connector 패키지 추가
- Spark Streaming Ivy cache 설정 추가
- Spark Gold Executor 2개로 복구
- restartPolicy: Always로 변경 (Executor 자동 재생성)
```

**변경 파일:**
- `K8s/clickhouse-schema.sql`
- `K8s/was-deployment.yaml`
- `K8s/kafka-strimzi.yaml`
- `K8s/spark-app.yaml`
- `K8s/spark-gold-application.yaml`

---

## 5. ConfigMap 설명

### ConfigMap이란?
- Kubernetes에서 **설정 파일이나 코드를 Pod에 전달**하는 방법
- Docker 이미지 다시 빌드 안 해도 코드/설정 변경 가능

### 우리 프로젝트의 ConfigMap

| ConfigMap 이름 | 용도 | 마운트 경로 |
|----------------|------|-------------|
| spark-app-code | Spark Streaming 코드 | /opt/spark/code/ |
| spark-gold-code | Spark Gold 코드 | /opt/spark/code/ |

### 사용 방법
```bash
# ConfigMap 수정 후 적용
kubectl apply -f spark-code-configmap.yaml

# Pod 재시작 (새 ConfigMap 반영)
kubectl delete pod spark-streaming
kubectl apply -f spark-direct.yaml
```

---

---

### 2.6 ClickHouse 스키마 퍼널 테이블 불일치 수정

**문제:**
- ClickHouse 스키마: `metrics_funnel_hourly` (hour_bucket 컬럼)
- Spark Gold 코드: `metrics_funnel_daily` (date 컬럼)
- 테이블 이름과 컬럼 이름 불일치로 데이터 적재 실패

**원인:**
- Spark 레포(`Back-to-the-Eng/Spark`)의 `clickhouse/schema.sql`과 실제 코드(`clickhouse_gold_connector.py`)가 불일치
- K8s 스키마도 잘못된 테이블 이름 사용

**해결:**
```sql
-- 변경 전
CREATE TABLE IF NOT EXISTS logs.metrics_funnel_hourly
(
    hour_bucket DateTime,
    ...
)

-- 변경 후
CREATE TABLE IF NOT EXISTS logs.metrics_funnel_daily
(
    date Date,
    ...
)
```

**변경 파일:**
- `K8s/clickhouse-schema.sql`
- ClickHouse 실제 테이블 재생성

**커밋:** `710c9e2` - fix: ClickHouse 스키마 퍼널 테이블 수정

---

### 2.7 WAS CPU 무제한 사용 문제

**문제:**
- WAS Pod 3개가 총 약 2.2코어를 무제한으로 사용
- 노드 CPU가 100% 초과하여 다른 Pod 스케줄링 불가

**원인:**
- `was-deployment.yaml`에 리소스 제한(limits)이 없음
- CPU가 무제한으로 사용되어 노드 리소스 고갈

**해결:**
```yaml
resources:
  requests:
    cpu: "200m"
    memory: "512Mi"
  limits:
    cpu: "1000m"  # 1코어로 제한
    memory: "1Gi"
```

**효과:**
- WAS CPU 사용량: 약 2.2코어 → 최대 3코어로 제한
- 노드 CPU 사용량 정상화 (100% → 4-11%)

**변경 파일:** `K8s/was-deployment.yaml`

---

### 2.8 Kafka 리소스 제한 추가

**문제:**
- Kafka Broker와 Zookeeper에 리소스 제한 없음
- CPU/Memory 무제한 사용 가능

**해결:**
```yaml
kafka:
  resources:
    requests:
      cpu: "200m"
      memory: "1Gi"
    limits:
      cpu: "500m"
      memory: "2Gi"

zookeeper:
  resources:
    requests:
      cpu: "100m"
      memory: "512Mi"
    limits:
      cpu: "300m"
      memory: "1Gi"
```

**효과:**
- Kafka Broker CPU: 최대 500m × 3 = 1.5코어
- Zookeeper CPU: 최대 300m × 3 = 0.9코어
- 총 약 1-2코어 절약

**변경 파일:** `K8s/kafka-strimzi.yaml`

---

### 2.9 Spark Streaming Kafka Connector 누락

**문제:**
```
AnalysisException: Failed to find data source: kafka. 
Please deploy the application as per the deployment section of 
Structured Streaming + Kafka Integration Guide.
```

**원인:**
- Apache Spark 3.5.1 공식 이미지는 Kafka connector를 포함하지 않음
- 별도 Maven 패키지로 추가 필요

**해결:**
```yaml
sparkConf:
  "spark.jars.packages": "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1"
```

**변경 파일:** `K8s/spark-app.yaml`

---

### 2.10 Spark Streaming Ivy Cache 에러

**문제:**
```
FileNotFoundException: /home/spark/.ivy2.5.2/cache/...
```

**원인:**
- Spark Streaming에도 동일한 Ivy cache 권한 문제
- Spark Gold와 동일한 해결 방법 필요

**해결:**
```yaml
sparkConf:
  "spark.jars.ivy": "/tmp/.ivy2"
  "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
  "spark.executor.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
```

**변경 파일:** `K8s/spark-app.yaml`

---

### 2.11 Spark Gold Executor 1개 → 2개 복구

**문제:**
- CPU 부족으로 Spark Streaming 실행을 위해 Executor를 1개로 줄임
- 분산 처리 불가능 (Executor 1개 = 분산 처리 아님)

**해결:**
- 리소스 제한 적용 후 CPU 여유 생김
- Executor를 2개로 복구하여 분산 처리 가능

```yaml
executor:
  cores: 1
  memory: "2g"
  instances: 2  # Executor 2개로 분산 처리 (노드 4개 활용)
```

**변경 파일:** `K8s/spark-gold-application.yaml`

---

### 2.12 Executor가 죽었을 때 재생성 안되는 문제

**문제:**
- SparkApplication의 `restartPolicy: OnFailure`는 Driver 실패 시만 재시작
- Executor가 죽으면 재생성되지 않음
- Deployment/StatefulSet과 달리 SparkApplication은 Executor Pod를 직접 관리하지 않음

**원인:**
- SparkApplication은 Executor Pod를 직접 관리하지 않음
- Executor는 Driver가 관리하는데, Executor가 죽으면 Driver가 재생성해야 하는데 그게 안됨

**해결:**
```yaml
# 1. restartPolicy를 Always로 변경
restartPolicy:
  type: Always  # OnFailure → Always
  onFailureRetries: 3
  onFailureRetryInterval: 10

# 2. Spark Gold에 동적 Executor 할당 추가 (배치 작업)
sparkConf:
  "spark.dynamicAllocation.enabled": "true"
  "spark.dynamicAllocation.minExecutors": "2"
  "spark.dynamicAllocation.maxExecutors": "4"
  "spark.dynamicAllocation.initialExecutors": "2"
  "spark.dynamicAllocation.executorIdleTimeout": "60s"
  
# 3. Executor 실패 처리 설정
  "spark.task.maxAttempts": "4"
  "spark.kubernetes.executor.lostCheckInterval": "30"
```

**효과:**
- Executor가 죽으면 자동으로 재생성
- Spark Gold는 동적 할당으로 Executor 자동 관리
- Spark Streaming은 restartPolicy: Always로 모니터링

**변경 파일:**
- `K8s/spark-gold-application.yaml`
- `K8s/spark-app.yaml`

---

### 2.13 파드가 자꾸 죽는 문제 (DiskPressure)

**문제:**
- 노드들이 DiskPressure 상태
- Kubelet이 파드를 Evict (쫓아냄)
- 파드가 재시작됨

**원인:**
- 노드 디스크 공간 부족
- 이미지 가비지 컬렉션 실패

**해결:**
- PVC 적용으로 데이터 영속성 확보 (MinIO, ClickHouse)
- 리소스 제한 추가로 CPU/Memory 관리
- 디스크 압박 해소됨 (현재 정상)

**현재 상태:**
- 모든 Pod 정상 Running
- 재시작 이력 없음

---

## 6. 다음 작업

- [x] 백로그 처리 완료 대기 또는 Silver 버킷 리셋
- [x] ClickHouse 데이터 적재 확인
- [x] Grafana 대시보드 연동 확인
- [x] 리소스 제한 추가
- [x] Executor 자동 재생성 설정

---

## 7. 참고 명령어

```bash
# 전체 Pod 상태 확인
kubectl get pods --all-namespaces

# Spark Gold 로그 확인
kubectl logs spark-gold-cluster-driver -n default --tail=20

# Spark Streaming 로그 확인
kubectl logs spark-streaming -n default --tail=20

# MinIO 버킷 확인
kubectl exec -it $(kubectl get pods -n storage -l app=minio -o jsonpath='{.items[0].metadata.name}') -n storage -- \
  sh -c "mc alias set myminio http://localhost:9000 admin password1234 && mc ls myminio/"

# SparkApplication 상태 확인
kubectl get sparkapplication -n default

# 노드 CPU 사용량 확인
kubectl top nodes

# ClickHouse 테이블 데이터 확인
kubectl exec $(kubectl get pods -n storage -l app=clickhouse -o jsonpath='{.items[0].metadata.name}') -n storage -- \
  clickhouse-client --password backtoeng -q "SELECT count() FROM logs.metrics_funnel_daily"

# 리소스 제한 확인
kubectl get deployment was-deployment -n was -o jsonpath='{.spec.template.spec.containers[0].resources}' | jq '.'
```

---

## 8. 오늘 해결한 문제 전체 요약

### 8.1 해결 완료된 문제 (13개)

| # | 문제 | 해결 방법 | 상태 |
|---|------|----------|------|
| 1 | Bronze 레이어 저장 누락 | Bronze writeStream 코드 추가 | ✅ |
| 2 | 체크포인트 PVC → S3 마이그레이션 | 체크포인트 경로를 S3로 변경 | ✅ |
| 3 | Ivy 캐시 에러 (Spark Gold) | Ivy cache 경로를 /tmp로 변경 | ✅ |
| 4 | Executor 리소스 최적화 | cores: 2 → 1, instances: 3 → 2 | ✅ |
| 5 | ClickHouse 스키마 퍼널 테이블 불일치 | metrics_funnel_hourly → metrics_funnel_daily | ✅ |
| 6 | WAS CPU 무제한 사용 | 리소스 제한 추가 (200m-1000m) | ✅ |
| 7 | Kafka 리소스 제한 없음 | Broker/Zookeeper 리소스 제한 추가 | ✅ |
| 8 | Spark Streaming Kafka connector 누락 | spark.jars.packages 추가 | ✅ |
| 9 | Spark Streaming Ivy cache 에러 | Ivy cache 경로 설정 추가 | ✅ |
| 10 | Spark Gold Executor 1개 | 2개로 복구 (분산 처리) | ✅ |
| 11 | Executor 죽었을 때 재생성 안됨 | restartPolicy: Always + 동적 할당 | ✅ |
| 12 | 파드가 자꾸 죽는 문제 | 리소스 제한 + PVC 적용 | ✅ |
| 13 | 노드 CPU 과부하 | 리소스 제한으로 정상화 (100% → 4-11%) | ✅ |

### 8.2 남은 문제

| 문제 | 상태 | 비고 |
|------|------|------|
| Spark Streaming | PENDING_RERUN | Driver 계속 실패 (별도 원인 확인 필요) |

### 8.3 최종 시스템 상태

**Pod 상태:**
- 전체 12개 Pod 모두 Running ✅
- Spark Gold Executor 2개 모두 Running ✅
- 재시작 이력 없음 ✅

**데이터 상태:**
- ClickHouse: 모든 테이블에 데이터 있음 ✅
  - metrics_server_health_minutely: 11
  - metrics_operational_hourly: 20
  - metrics_business_daily: 108
  - metrics_funnel_daily: 1 ✅ (수정 완료)
  - metrics_data_quality_hourly: 2
- MinIO: Bronze, Silver, Gold 모두 데이터 있음 ✅

**리소스 상태:**
- 노드 CPU: 정상 (4-11%) ✅
- PVC: 모두 Bound (3개) ✅
- 리소스 제한: WAS, Kafka 적용 완료 ✅

**설정 상태:**
- restartPolicy: Always (Executor 자동 재생성) ✅
- 동적 Executor 할당: Spark Gold 활성화 ✅

---

## 9. 교훈 및 주의사항

### 9.1 SparkApplication vs Deployment/StatefulSet

| 관리 방식 | Pod 죽으면? |
|-----------|-------------|
| **Deployment/StatefulSet** | 자동 재생성 ✅ |
| **SparkApplication** | Executor는 재생성 안됨 ❌ |

**해결:**
- `restartPolicy: Always`로 변경
- Spark Gold는 동적 Executor 할당 활성화

### 9.2 Apache Spark 공식 이미지 제한사항

- **Kafka connector 미포함**: 별도 Maven 패키지 추가 필요
- **Ivy cache 권한 문제**: `/tmp` 경로 사용 필요

### 9.3 리소스 제한의 중요성

- 리소스 제한 없으면 CPU/Memory 무제한 사용
- 노드 리소스 고갈 → 다른 Pod 스케줄링 불가
- **모든 Deployment에 리소스 제한 필수!**

### 9.4 스키마와 코드 일치 확인

- ClickHouse 스키마와 Spark 코드의 테이블/컬럼 이름 일치 확인 필수
- GitHub 레포와 K8s 레포 모두 확인 필요

---
