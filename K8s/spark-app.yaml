apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-kafka-consumer
  namespace: default
spec:
  type: Python
  mode: cluster
  sparkVersion: "3.5.1"

  image: "apache/spark:3.5.1-scala2.12-java11-python3-ubuntu"
  imagePullPolicy: Always

  mainApplicationFile: "local:///opt/spark/code/user_activity_streaming.py"

  sparkConf:
    "spark.kubernetes.container.image.pullPolicy": "Always"
    "spark.jars.packages": "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"
    "spark.jars.ivy": "/tmp/.ivy2"
    "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
    "spark.executor.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp -Divy.default.ivy.user.dir=/tmp"
    # Executor 실패 시 자동 재생성 설정
    "spark.task.maxAttempts": "4"
    "spark.kubernetes.executor.lostCheckInterval": "30"
    # Spark Streaming은 동적 할당 미지원이므로 제거
    # Checkpoint Race Condition 방지 설정
    "spark.sql.streaming.fileSource.log.compactInterval": "10"
    "spark.sql.streaming.fileSource.log.cleanupDelay": "1h"
    "spark.sql.streaming.checkpointLocation.writeTimeout": "600s"
    "spark.sql.streaming.checkpointLocation.readTimeout": "600s"

  restartPolicy:
    type: Always  # Always로 변경하여 Executor가 죽어도 재생성
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 3
    onSubmissionFailureRetryInterval: 10

  driver:
    cores: 1
    coreRequest: "800m"  # CPU 요청을 줄여서 리소스 효율성 향상
    memory: "1g"
    serviceAccount: spark
    labels:
      version: "3.5.1"
    volumeMounts:
    - name: spark-code
      mountPath: /opt/spark/code
    env:
      - name: KAFKA_BOOTSTRAP_SERVERS
        value: "streaming-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"
      - name: PYTHONPATH
        value: "/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip"

  executor:
    cores: 1
    coreRequest: "500m"
    memory: "1g"
    instances: 2  # Executor 2개로 조정
    labels:
      version: "3.5.1"
    volumeMounts:
    - name: spark-code
      mountPath: /opt/spark/code
    env:
      - name: KAFKA_BOOTSTRAP_SERVERS
        value: "streaming-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"
      - name: PYTHONPATH
        value: "/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip"

  volumes:
  - name: spark-code
    configMap:
      name: spark-app-code
