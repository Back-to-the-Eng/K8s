# Spark Streaming 멀티 인스턴스 테스트용
# 사용법: 
# 1. spark-direct-multi-1.yaml, spark-direct-multi-2.yaml, spark-direct-multi-3.yaml로 복사
# 2. 각 파일에서 name과 consumer group을 변경
# 3. kubectl apply -f spark-direct-multi-1.yaml 등으로 적용

apiVersion: v1
kind: Pod
metadata:
  name: spark-streaming-multi-1
  labels:
    name: spark-streaming-multi-1
spec:
  serviceAccountName: spark
  containers:
  - name: spark-streaming
    image: doyoomii/spark-k8s:latest
    imagePullPolicy: Always
    command: ["/bin/bash", "-c"]
    args:
      - |
        /opt/spark/bin/spark-submit \
        --master k8s://https://kubernetes.default.svc \
        --deploy-mode client \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
        --conf spark.kubernetes.namespace=default \
        --conf spark.app.name=spark-streaming-multi-1 \
        --conf spark.kubernetes.container.image=doyoomii/spark-k8s:latest \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 \
        --conf spark.sql.streaming.checkpointLocation=s3a://mybucket/checkpoint/spark-streaming-multi-1 \
        --conf spark.hadoop.fs.s3a.endpoint=http://minio.storage.svc.cluster.local:9000 \
        --conf spark.hadoop.fs.s3a.access.key=admin \
        --conf spark.hadoop.fs.s3a.secret.key=password1234 \
        --conf spark.hadoop.fs.s3a.path.style.access=true \
        --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
        --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
        --conf spark.sql.streaming.kafka.consumer.group.id=spark-streaming-multi-1 \
        /app/user_activity_streaming.py
    env:
    - name: KAFKA_BOOTSTRAP_SERVERS
      value: streaming-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092
    - name: AWS_ACCESS_KEY_ID
      value: admin
    - name: AWS_SECRET_ACCESS_KEY
      value: password1234
    - name: AWS_REGION
      value: ap-northeast-2
    volumeMounts:
    - name: spark-code
      mountPath: /app
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "500m"
  volumes:
  - name: spark-code
    configMap:
      name: spark-code-configmap
